Simple Join in Spark:

1. Open pyspark shell

[cloudera@quickstart ~]$ sudo easy_install ipython==1.2.1
[cloudera@quickstart ~]$ PYSPARK_DRIVER_PYTHON=ipython pyspark
In [1]: sc.version
Out[1]: u'1.6.0'


2. Load dataset FileA from HDFS:

In [9]: fileA = sc.textFile("input/join1_FileA.txt")
In [10]: fileA.collect()
Out[10]: [u'able,991', u'about,11', u'burger,15', u'actor,22']

3. Load the 2nd dataset from HDFS:

In [11]: fileB = sc.textFile("input/join1_FileB.txt")
In [12]: fileB.collect()
Out[12]: 
[u'Jan-01 able,5',
 u'Feb-02 about,3',
 u'Mar-03 about,8',
 u'Apr-04 able,13',
 u'Feb-22 actor,3',
 u'Feb-23 burger,5',
 u'Mar-08 burger,2',
 u'Dec-15 able,100']

4. Mapper for FileA

You need to creat a map function for FileA that takes a line, splits it on the comma and turns the count to an integer.

In [15]: def split_fileA(line): # split the input line in word and count on the comma
   ....:     line = line.strip()       # strip out carriage return
   ....:     key_value  = line.split(',')  # split line into key and value, returns a lis
   ....:     word = key_value[0]
   ....:     count = int(key_value[1])
   ....:     return(word, count)
   ....: 

In [16]: test_lineA = "able, 911"

In [17]: split_fileA(test_lineA)
Out[17]: ('able', 911)

5. Now we can proceed on running the map transformation to the fileA RDD:
 
In [18]: fileA_data = fileA.map(split_fileA)

In [19]: fileA_data.collect()
Out[19]: [(u'able', 991), (u'about', 11), (u'burger', 15), (u'actor', 22)]

6. Mapper for fileB

The is more complex because we need to extracct date and put date in value.

In [24]: def split_fileB(line):
   ....:     line = line.strip()
   ....:     date_word_count = line.split(",")
   ....:     date_word = date_word_count[0]
   ....:     count = date_word_count[1]
   ....:     date_word = date_word.split(" ")
   ....:     date = date_word[0]
   ....:     word = date_word[1]
   ....:     return (word, date + " " + count)
   ....: 

In [25]: test_lineB = "jan-01 able,5"

In [26]: split_fileB(test_lineB)
Out[26]: ('able', 'jan-01 5')

In [27]: fileB_data = fileB.map(split_fileB)

and then gather the output back to the pyspark driver console:

In [28]: fileB_data.collect()

Out[28]: 
[(u'able', u'Jan-01 5'),
 (u'about', u'Feb-02 3'),
 (u'about', u'Mar-03 8'),
 (u'able', u'Apr-04 13'),
 (u'actor', u'Feb-22 3'),
 (u'burger', u'Feb-23 5'),
 (u'burger', u'Mar-08 2'),
 (u'able', u'Dec-15 100')]

7. Run join

The goal is to join the two datasets using the words as keys and print for each word the wordcount for a specific date and then the total output from A.

Basically, for each word in fileB, we would like to print the date and count from fileB but also the total count from fileA.

Spark implements the join transformation given a RDD of (K, V) pairs to be joined with another RDD of (K, W) pairs, returns a dataset that contains (K, (V, W)) pairs.

In [29]: fileB_joined_fileA = fileB_data.join(fileA_data)

In [30]: fileB_joined_fileA.collect()
Out[30]: 
[(u'able', (u'Jan-01 5', 991)),
 (u'able', (u'Apr-04 13', 991)),
 (u'able', (u'Dec-15 100', 991)),
 (u'burger', (u'Feb-23 5', 15)),
 (u'burger', (u'Mar-08 2', 15)),
 (u'about', (u'Feb-02 3', 11)),
 (u'about', (u'Mar-03 8', 11)),
 (u'actor', (u'Feb-22 3', 22))]
